#-------------------------------------------------------------------------------
# stream reddit
#-------------------------------------------------------------------------------
<source>
  @type tail
  path /data/reddit-comments/2015/*
  pos_file /data/reddit-comments/reddit-comments.pos
  tag reddit_2015
  format json
  message_key body
  #time_key created_utc
</source>
<source>
  @type tail
  path /data/reddit-comments/2014/*
  pos_file /data/reddit-comments/reddit-comments.pos
  tag reddit_2014
  format json
  message_key body
  #time_key created_utc
</source>
<source>
  @type tail
  path /data/reddit-comments/2013/*
  pos_file /data/reddit-comments/reddit-comments.pos
  tag reddit_2013
  format json
  #time_key created_utc
  message_key body
</source>
<source>
  @type tail
  path /data/reddit-comments/2012/*
  pos_file /data/reddit-comments/reddit-comments.pos
  tag reddit_2012
  format json
  message_key body
  #time_key created_utc
</source>

# http://docs.fluentd.org/articles/filter-modify-apache
# http://docs.fluentd.org/articles/filter_grep
<match reddit_2*>
	@type grep
	regexp1 subreddit_id (2xtuc|2qh9z|2yv5q|33jry|2qhff|2sodo|39dpu|2s4ni|2qwut|2rdms|2qqqf|2qnwb|2wk9n|2rghq)
	exclude1 body 		".+deleted.+"
	tag filtered_reddit2
</match>

<match filtered_reddit2>
  type copy
  <store>
    @type               	kafka_buffered
    brokers             	ip-172-31-1-136:9092,ip-172-31-1-137:9092,ip-172-31-1-133:9092,ip-172-31-1-139:9092 # Set brokers directly
    default_topic       	reddit4
    buffer_type				file
    buffer_path				/data/reddit-comments/
    output_data_type    	json #(json|ltsv|msgpack|attr:<record name>|<formatter name>)
    max_send_retries    	3 #(integer)    :default => 3
    required_acks       	0 #(integer)    :default => 0
    ack_timeout_ms      	1500 #(integer)    :default => 1500
    compression_codec   	none #(none|gzip|snappy) :default => none
  </store>
</match>

#-------------------------------------------------------------------------------
# produce a simulated bid stream by java app, and spark workers from python job
#-------------------------------------------------------------------------------
<source>
	@type forward
	port 24224
</source>
<match pharma.bids.prices>
	type copy 
	<store>
	    type                kafka
	    brokers             52.33.29.117:9092,52.33.248.41:9092,52.35.99.109:9092,52.89.231.174:9092 # Set brokers directly
	    default_topic       pharma_bids_prices2
	    output_data_type    json #(json|ltsv|msgpack|attr:<record name>|<formatter name>)
	    max_send_retries    3 #(integer)    :default => 3
	    required_acks       0 #(integer)    :default => 0
	    ack_timeout_ms      1500 #(integer)    :default => 1500
	    compression_codec   none #(none|gzip|snappy) :default => none
	</store>
</match>
<match spark.out.*>
	type copy
	 <store>
		@type  				elasticsearch
		logstash_format 	true
		hosts 				ec2-52-89-90-56.us-west-2.compute.amazonaws.com:9200,ec2-52-88-121-199.us-west-2.compute.amazonaws.com:9200,ec2-52-89-89-147.us-west-2.compute.amazonaws.com:9200,ec2-52-88-7-3.us-west-2.compute.amazonaws.com:9200
		logstash_dateformat %Y.%m
		port 				9200
		flush_interval		60
		id_key				id
  	</store>
</match>

#-----------------
# twitter producer
# https://github.com/fluent/fluentd/blob/master/lib/fluent/plugin/in_tail.rb
#------------------
<source>
  @type 					tail
  #path 						/data/TT.raw/joyce-raw-twitter/TT-logs0129/events/ts=%Y%m%d-*/*
  path 						/data/TT.raw/joyce-raw-twitter/TT-logs0129/events/ts*/*
  pos_file 					/data/TT.raw/joyce-raw-twitter/joyce-raw-twitter.pos
  tag 						TT.raw.frombackup
  format 					json
  read_from_head			true
  message_key 				body
</source>

<source>
  type twitter
  consumer_key        		sdfadf       # Required
  consumer_secret     		aweradfasdf    # Required
  oauth_token         		3255564181- sfd      # Required
  oauth_token_secret  		sdfdsef # Required
  tag                 		TT.raw  # Required
  timeline            		sampling                # Required (tracking or sampling or location or userstream)
  lang                		en                     # Optional
  output_format       		flat                   # Optional (nest or flat or simple[default])
</source>

<match TT.raw>
  type copy
  <store>
    @type 					s3
    aws_key_id 				
    aws_sec_key 			+8TgEMbTWpJcl6/Cd3Cs8QOb1Jf
    s3_object_key_format 	%{path}/events/ts=%{time_slice}/events_%{index}.%{file_extension}
    s3_bucket 				joyce-raw-twitter
    s3_region 				us-west-2
    path 					TT-logs0129
    buffer_path 			/data/TT.raw/s3
    time_slice_format 		%Y%m%d-%H
    time_slice_wait 		10m
    utc
    format 					json
    store_as 				json
    buffer_chunk_limit 		180m
    flush_interval 			1h
  </store>
  <store>
    type                	kafka
    brokers             	52.33.29.117:9092,52.33.248.41:9092,52.35.99.109:9092,52.89.231.174:9092 # Set brokers directly
    default_topic       	TT_raw
    output_data_type    	json #(json|ltsv|msgpack|attr:<record name>|<formatter name>)
    max_send_retries    	3 #(integer)    :default => 3
    required_acks       	0 #(integer)    :default => 0
    ack_timeout_ms      	1500 #(integer)    :default => 1500
    compression_codec   	none #(none|gzip|snappy) :default => none
  </store>
</match>

<match TT.raw.frombackup>
  type copy
  #<store>
  #	type 					stdout
  #</store>
  <store>
    type                	kafka
    brokers            		52.33.29.117:9092,52.33.248.41:9092,52.35.99.109:9092,52.89.231.174:9092 # Set brokers directly
    default_topic       	TT_raw
    output_data_type    	json #(json|ltsv|msgpack|attr:<record name>|<formatter name>)
    max_send_retries    	3 #(integer)    :default => 3
    required_acks       	0 #(integer)    :default => 0
    ack_timeout_ms      	1500 #(integer)    :default => 1500
    compression_codec   	none #(none|gzip|snappy) :default => none
  </store>
</match>
